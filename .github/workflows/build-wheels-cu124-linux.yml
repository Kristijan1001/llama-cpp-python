name: Build Wheels(CU124) for Linux # Workflow name

on:
  workflow_dispatch: # Manual trigger

permissions:
  contents: write

jobs:
  build_wheels:
    name: Build Wheel ${{ matrix.os }} ${{ matrix.pyver }} ${{ matrix.cuda }} ${{ matrix.releasetag == 'wheels' && 'AVX2' || matrix.releasetag }}
    runs-on: ubuntu-22.04
    container: nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04
    strategy:
      matrix: # Define the build matrix directly here
        os: ["ubuntu-22.04"]
        pyver: ["3.10", "3.11", "3.12"] # Python versions
        cuda: ["12.4.1"]
        releasetag: ["AVX2"] # Controls CMAKE_ARGS for CPU features (even in CUDA build)
        cudaarch: ["all"] # Controls target CUDA architectures for nvcc

    defaults:
      run:
        shell: bash

    env:
      CUDAVER: ${{ matrix.cuda }}
      AVXVER: ${{ matrix.releasetag }}
      CUDAARCHVER: ${{ matrix.cudaarch }}

    steps:
      - name: Install dependencies
        run: |
            apt update
            apt install -y build-essential cmake curl git libgomp1 libcurl4-openssl-dev

      - uses: actions/checkout@v4 # Checkout code
        with:
          submodules: "recursive"

      # from astral-sh/setup-uv
      - name: Install the latest version of uv and set the python version
        uses: astral-sh/setup-uv@v6
        with:
          python-version: ${{ matrix.pyver }}
          activate-environment: true
          enable-cache: true

      - run: nvcc -V

      - name: Build Wheel With Cmake # Main build step: configures and builds the wheel
        env:
          LD_LIBRARY_PATH: "/usr/local/cuda/lib64:/usr/local/cuda/compat:/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}"
          VERBOSE: 1 # Enable verbose build output
          CUDA_HOME: "/usr/local/cuda/" # Set CUDA_HOME
          CUDA_PATH: "${PATH}"
          CUDA_TOOLKIT_ROOT_DIR: "/usr/local/cuda/" # Set CUDA_TOOLKIT_ROOT_DIR
        run: |
          echo "VERBOSE=1" >> $GITHUB_ENV # Enable verbose build output for troubleshooting
          find /usr/ -name 'libcuda.so.*'
          echo $LD_LIBRARY_PATH

          # Add project-specific and feature flags
          CMAKE_ARGS="-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES='70-real;75-real;80-real;86-real;89-real'"
          CMAKE_ARGS="-DGGML_CUDA_FORCE_MMQ=ON ${CMAKE_ARGS}"
          CMAKE_ARGS="${CMAKE_ARGS} -DLLAMA_CURL=ON"

          if [ "${AVXVER}" = "AVX" ]; then
            CMAKE_ARGS="${CMAKE_ARGS} -DGGML_AVX2=off -DGGML_FMA=off -DGGML_F16C=off"
          fi
          if [ "${AVXVER}" = "AVX2" ]; then
            CMAKE_ARGS="${CMAKE_ARGS} -DGGML_AVX2=on -DGGML_FMA=off -DGGML_F16C=off"
          fi
          # if [ "${AVXVER}" = "AVX512" ]; then
          #   CMAKE_ARGS="${CMAKE_ARGS} -DGGML_AVX512=on"
          # fi
          # if [ "${AVXVER}" = "basic" ]; then
          #   CMAKE_ARGS = "${CMAKE_ARGS} -DGGML_AVX=off -DGGML_AVX2=off -DGGML_FMA=off -DGGML_F16C=off"
          # fi

          # Export CMAKE_ARGS environment variable so the python -m build command can use it
          echo ${CMAKE_ARGS}
          echo "CMAKE_ARGS=${CMAKE_ARGS}" >> $GITHUB_ENV

          # Run the Python build command to generate the wheel
          uv pip install build setuptools wheel packaging
          # uv pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
          CMAKE_ARGS=${CMAKE_ARGS} uv build --wheel

          # --- Post-build steps to get info for release tag ---

          # Find the generated wheel file in the 'dist' directory using bash
          # Assumes only one wheel is generated per build configuration run
          wheel_file=$(ls dist/*.whl | head -n 1)

          # Extract the package version (e.g., 1.2.3) from the wheel filename
          # Filename format is typically: package_name-version-tag-specificators.whl
          # Using basename and cut to split by '-' and get the second field
          tag_ver=$(basename "$wheel_file" | cut -d'-' -f 2)
          echo "TAG_VERSION=$tag_ver" >> $GITHUB_ENV # Store version in env for release step

          # Extract the short CUDA version (e.g., 124) from the full version (e.g., 12.4.1) from the matrix variable
          cuda_ver_short=$(echo "${CUDAVER}" | cut -d'.' -f 1,2 | sed 's/\.//g')
          echo "CUDA_VERSION=$cuda_ver_short" >> $GITHUB_ENV # Store short CUDA version in env


      - name: Get Current Date # Step to get current date for the release tag
        id: get-date
        run: |
          # Get date in YYYYMMDD format using bash date command
          currentDate=$(date +%Y%m%d)
          # Store the date in environment variable for the release step
          echo "BUILD_DATE=$currentDate" >> $GITHUB_ENV

      - uses: softprops/action-gh-release@v2 # Action to create a GitHub Release
        with:
          files: dist/* # Upload the generated wheel files from the dist directory
          # Define the release tag name using the collected environment variables
          # Format: v<package_version>-cu<short_cuda_version>-<avx_tag>-linux-<build_date>
          tag_name: v${{ env.TAG_VERSION }}-cu${{ env.CUDA_VERSION }}-${{ env.AVXVER }}-linux-${{ env.BUILD_DATE }} # Release tag format for Linux
          # Note: This action will create a new release tag if it doesn't exist,
          # or upload assets to an existing tag. Be mindful of potential tag name conflicts.
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # Use the secret provided by GitHub Actions for authentication